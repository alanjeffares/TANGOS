**Comment on functorch and batch norm**

The original implementation of TANGOS used a different, less efficient method for the calculation of gradient attributions. This was updated after running the in tandem experiments. Although both methods produce identical results, there is a compatibility issue between functorch (the new library for calculating attributions) and batch norm. More details on this issue are discussed [here](https://pytorch.org/functorch/stable/batch_norm.html) and [here](https://github.com/pytorch/functorch/issues/384). We have therefore removed the combination of TANGOS and batch norm from the config for this experiment by default. In case this particular combination is required by someone in the future, we have included [the original implementation](https://github.com/alanjeffares/TANGOS/blob/main/src/legacy/legacy.py) for calculating the attribution loss in this folder.

